import streamlit as st
import pandas as pd
import numpy as np
import re
import nltk
import joblib
import emoji
import os
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
st.set_page_config(page_title="Sentiment Analysis", layout="centered")
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
def preprocess(text):
    text = emoji.demojize(text)  # Convert emojis to text
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)  # URLs
    text = re.sub(r"\@\w+|\#", '', text)  # mentions & hashtags
    text = re.sub(r"[^a-zA-Z\s]", '', text)  # special characters
    text = text.lower()
    words = text.split()
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return " ".join(words)
st.sidebar.title("üìÇ Options")
data_option = st.sidebar.radio("Load Data Option", ("Use Sample", "Upload File"))
@st.cache_data
def load_sample_data():
    file_path = r"C:\Users\yp104\Desktop\t\training.1600000.processed.noemoticon.csv\training.1600000.processed.noemoticon.csv"
    df = pd.read_csv(file_path, encoding='latin-1', header=None)
    df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']
    df = df[['target', 'text']]
    df['target'] = df['target'].apply(lambda x: 1 if x == 4 else 0)
    df = df.sample(n=100000, random_state=42)
    df['clean_text'] = df['text'].apply(preprocess)
    return df
if data_option == "Use Sample":
    df = load_sample_data()
elif data_option == "Upload File":
    uploaded_file = st.sidebar.file_uploader("Upload CSV", type="csv")
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file, encoding='latin-1', header=None)
        df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']
        df = df[['target', 'text']]
        df['target'] = df['target'].apply(lambda x: 1 if x == 4 else 0)
        df = df.sample(n=100000, random_state=42)
        df['clean_text'] = df['text'].apply(preprocess)
    else:
        st.warning("Please upload a file to proceed.")
        st.stop()
st.subheader("üìä Sample Data")
st.dataframe(df[['text', 'target']].head())
st.subheader("üîÅ Training Model...")
vectorizer = TfidfVectorizer(max_features=7000, ngram_range=(1, 2))
X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['target'], test_size=0.2, random_state=42)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)
model = LogisticRegression(max_iter=1000)
model.fit(X_train_vec, y_train)
accuracy = model.score(X_test_vec, y_test)
st.success(f"‚úÖ Model trained with accuracy: **{accuracy:.2%}**")
joblib.dump(model, 'sentiment_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

def predict_sentiment(text):
    clean = preprocess(text)
    vec = vectorizer.transform([clean])
    pred = model.predict(vec)[0]
    return "Positive üòä" if pred == 1 else "Negative üòû"

st.subheader("üí¨ Try It Yourself")
user_input = st.text_input("Enter your sentence here:")
if user_input:
    prediction = predict_sentiment(user_input)
    st.markdown(f"**Sentiment:** {prediction}")

st.subheader("üìë Predict Multiple Sentences (Optional)")
multi_text = st.text_area("Enter multiple sentences (one per line):")
if multi_text:
    lines = multi_text.strip().split('\n')
    results = [predict_sentiment(line) for line in lines]
    for sent, res in zip(lines, results):
        st.write(f"**{sent}** ‚û°Ô∏è {res}")
